{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is the process of looking for patterns, differences, and other features within a dataset. You are trying to analyze your dataset to summarize its main characteristics.\n",
    "\n",
    "EDA can be used to check your data for inconsistencies or identify limitations in the data, as well as identifying trends or outliers in the data.\n",
    "\n",
    "EDA can be done to generate ideas or hypotheses about your population of interest.\n",
    "\n",
    "EDA is often done visually, by creating plots, or numerically, by calculating **descriptive statistics**.\n",
    "\n",
    "The tools that you use depend on whether you are looking at numeric or categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Variables\n",
    "\n",
    "You'll start by exploring a numerical variable. Specifically, you'll be looking at the salaries and the heights of the players for the 2019-2020 Los Angeles Lakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "**Goal:** Describe a data set in terms of its important features. Summarize a data set using just a few numbers and/or a plot. Rather than looking at the entire dataset at once, convert it into an easily digestible form.\n",
    "\n",
    "There are three major categories of descriptive statistics:\n",
    "* Measures of Central Tendency\n",
    "* Measures of Variability/Spread\n",
    "* Measures of Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Central Tendency\n",
    "\n",
    "Give a central or \"typical\" value of a data set.\n",
    "\n",
    "Most common measures of central tendency:\n",
    "* mean\n",
    "* median\n",
    "* mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean\n",
    "\n",
    "Also known as the *average* or *arithmetic mean*. Defined as total (sum) of the values of a set of observations divided by the number of observations. When calculating the mean, the only difference between a sample and population mean is the notation.\n",
    "\n",
    "$$\\text{Sample Mean: } \\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum\\limits_{i=1}^n x_i}{n}$$\n",
    "\n",
    "$$\\text{Population Mean: } \\mu = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum\\limits_{i=1}^n x_i}{n}$$\n",
    "\n",
    "The mean represents the “balance point” of the data. \n",
    "\n",
    "It is the amount that all observations would have if the total amount of the variable was evenly distributed to all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be using the `pandas` and `numpy` libraries to perform some of your calculations and the `pyplot` module from `matplotlib`. You must first import these library. You will *alias* `pandas` as `pd` so that when you refer to it later, you only need to type `pd`. Similarly, you will alias `numpy` and `matplotlib.pyplot`. Finally, you'll add the `%matplotlib inline` *magic* so that your plots will display in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll read in the dataset from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers = pd.read_csv('../data/Lakers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll manually calculate the mean, so that you can see how to use some of the methods available in `pandas`. \n",
    "\n",
    "To use a dataframe method, you normally type the name of the dataframe followed by a . and the name of the method you wish to use. \n",
    "\n",
    "Note also that when using methods, you need to put a set of parentheses after the name of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].sum() / lakers['salary'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` library has many of the common descriptive statistics available as methods. For example, to compute the mean salary, you didn't need to compute the sum and count, instead you could have taken advantage of the `mean` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that if you distributed the total payroll evenly to all players, they would each receive a salary of $7,857,737.\n",
    "\n",
    "Look back at the full dataset and notice that only 4 out of the 15 players made above the average, with the top three players making significantly higher than the average. This is typical for data sets where you have extreme observations.\n",
    "\n",
    "In this case, since Lebron James and Anthony Davis earn much higher salaries than the typical player, they end up pulling the average salary up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate the mean player height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort the table by height now to see how the mean compares to the overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.sort_values('height_inches').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the average height, being between the 8th and 9th observation (notice that Python starts counting at 0), is almost exactly in the middle of the overall distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Lebron James' salary is so much larger than the rest of the players, that it could be classified as an **outlier**. An outlier is a data point which differs significantly from other observations. It is either much higher or much lower than almost all others.\n",
    "\n",
    "There is no precise definition of an \"outlier\", and it often depends on particular data set you a looking at and on applying some domain knowledge of the problem at hand. We will see some common rule-of-thumbs for labeling observations as outliers in following sections.\n",
    "\n",
    "If a dataset contains outliers, certain descriptive statistics can be misleading. In our case, Lebron James' salary increases the mean so that only 4 players in the dataset have salaries larger than the mean salary.\n",
    "\n",
    "Depending on what you are doing with your dataset, you may or may not need to do anything besides note any points that may be outliers. When doing exploratory analysis, the main thing you need to do with outliers is take note of them and think about them. If you are looking at a sample trying to understand a larger population, you can think about the process that led to the outliers in your sample and what that means about the larger population.\n",
    "\n",
    "If you are concerned about outliers or think that there is a good chance that your dataset contains outliers, there are other descriptive statistics which are not as sensitive to them. Such statistics are termed as **robust** statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median\n",
    "\n",
    "The **median** is the number which divides the dataset exactly in half. It is the middle value if the data is arranged by size.\n",
    "\n",
    "For an odd number of observations, the median will be a value from the data set.\n",
    "\n",
    "For an even number of observations, the median is the mean of the two centermost observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `lakers` dataset, we have 15 observations, which means that the 8th observation corresponds to the median. In this case, Quinn Cook, who has a salary of $3,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the median height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mean and median heights are very close to each other, whereas the mean and median salaries are quite far apart. If the mean and median are quite different, this is often a sign of either the presence of outliers or that you are working with a *skewed* dataset. You'll learn more about skewed datasets later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimmed Mean\n",
    "\n",
    "Another option to reduce the effect of outliers is to use a **trimmed mean**. To compute the trimmed mean, you can remove the largest and smallest *x*% of observations and then calculate the mean. Often, the largest and smallest 10% are removed. This is a compromise between the mean and the median, in that it still takes into account a large portion of the dataset, while being less influeced by extreme observations.\n",
    "\n",
    "To calculate the trimmed mean, you can use the `trim_mean` function from the `scipy` `stats` module. To use this function, you need to pass in the data and specify the `proportiontocut` argument, as a decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_mean(lakers['salary'], proportiontocut=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in contrast to salaries, the 10% trimmed mean for heights is very close to the mean and median value. This can be attributed to the fact that there are a small number of players with very large salaries compared to the rest of the team. This is not the case for heights. We'll explore this idea more in the next section on distribution shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Shape\n",
    "\n",
    "When looking at a numerical variable, we can also inspect the *shape* of the distribution of that variable.\n",
    "\n",
    "The **distribution** refers to the possible values of that variable and which values occur more or less frequently than others.\n",
    "\n",
    "When talking about the shape of a distribution, there are a few different aspects we can examine:\n",
    "* **Symmetry:** Is the distribution symmetric? If so, is it \"bell-shaped\"? Is it flat?\n",
    "* **Skewness:** If it is not symmetic, does it have a long tail to one side?\n",
    "* **Peaks/Modes:** How many peaks does it have? Unimodal? Bimodal? Multimodal?\n",
    "* **Spread:** How narrow/wide is the distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "If you are trying to understand the shape of the distribution of a variable, the most common tool to use is the histrogram.\n",
    "\n",
    "A histogram shows how many observations lie within a certain class interval. That is, it divides the dataset into *bins*, and the height of the plot above each interval is proportional to the number of observations that fall within that bin.\n",
    "\n",
    "Procedure:\n",
    "* Separate data into equal-width, non-overlapping bins\n",
    "* Count number of data points in each bin\n",
    "\n",
    "When constructing a histogram, you must decide how many bins to use. A common rule of thumb is: $\\text{number of bins} = \\sqrt{\\text{sample size}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].hist()\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Salaries');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you need to adjust the number of bins in order to get a clearer idea of the distribution of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].hist(bins = 25)\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Salaries');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Lakers salary dataset, you can see that there are a few unusually large observations. Another way to say this is that the dataset has a long tail to the right. It is most definitely not symmetric.\n",
    "\n",
    "When a dataset has a long tail to the right, you say that it is **right-skewed**. \n",
    "\n",
    "Analogously, a dataset with a long tail to the left (unusually small observations) would be said to be **left-skewed**.\n",
    "\n",
    "For the distribution of salaries, there appears to be a single peak on the left, so this dataset would be classified as **unimodal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.salary.hist(bins = 25)\n",
    "plt.hlines(y = 0, xmin = 0, xmax = lakers['salary'].max() + 500000)\n",
    "plt.annotate(s = 'Mean', ha = 'center', va = 'top', fontweight = 'bold',\n",
    "             xy = (lakers['salary'].mean(), -.1), xytext = (lakers['salary'].mean(), -1), arrowprops=dict(width = 8, headwidth = 20, facecolor = 'red'))\n",
    "plt.ylim(-1.5, 6.3)\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Salaries');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].hist(bins = 25)\n",
    "plt.hlines(y = 0, xmin = 0, xmax = lakers.salary.max() + 500000)\n",
    "plt.annotate(s = 'Median', ha = 'center', va = 'top', fontweight = 'bold',\n",
    "             xy = (lakers['salary'].median(), -.1), xytext = (lakers['salary'].median(), -1), arrowprops=dict(width = 8, headwidth = 20, facecolor = 'red'))\n",
    "plt.ylim(-1.5, 6.3)\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Salaries');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.salary.hist(bins = 25)\n",
    "plt.hlines(y = 0, xmin = 0, xmax = lakers['salary'].max() + 500000)\n",
    "plt.annotate(s = 'Mean', ha = 'center', va = 'top', fontweight = 'bold',\n",
    "             xy = (lakers['salary'].mean(), -.1), xytext = (lakers['salary'].mean(), -1), arrowprops=dict(width = 8, headwidth = 20, facecolor = 'red'))\n",
    "plt.annotate(s = 'Median', ha = 'center', va = 'top', fontweight = 'bold',\n",
    "             xy = (lakers['salary'].median(), -.1), xytext = (lakers['salary'].median(), -1), arrowprops=dict(width = 8, headwidth = 20, facecolor = 'blue'))\n",
    "plt.ylim(-1.5, 6.3)\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Salaries');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also look at the distribution of heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['height_inches'].hist()\n",
    "plt.xlabel('height (in)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Heights');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small dataset, so it's hard to definitively classify the shape. It does not appear to be skewed, but is also not symmetric, since there are more values on the lower range than on the upper range.\n",
    "\n",
    "There are two peaks, one on either side, so it could be classified as a **bimodal** distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.height_inches.hist()\n",
    "plt.hlines(y = 0, xmin = lakers['height_inches'].min() - 0.5, xmax = lakers['height_inches'].max() + 0.5)\n",
    "plt.annotate(s = 'Mean', ha = 'center', va = 'top', fontweight = 'bold',\n",
    "             xy = (lakers['height_inches'].mean(), -.1), xytext = (lakers['height_inches'].mean(), -.75), arrowprops=dict(width = 8, headwidth = 20, facecolor = 'red'))\n",
    "plt.ylim(-1, 3.2)\n",
    "plt.xlabel('height (in)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Heights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.height_inches.hist()\n",
    "plt.hlines(y = 0, xmin = lakers['height_inches'].min() - 0.5, xmax = lakers['height_inches'].max() + 0.5)\n",
    "plt.annotate(s = 'Median', ha = 'center', va = 'top', fontweight = 'bold',\n",
    "             xy = (lakers['height_inches'].median(), -.1), xytext = (lakers['height_inches'].median(), -.75), arrowprops=dict(width = 8, headwidth = 20, facecolor = 'blue'))\n",
    "plt.ylim(-1, 3.2)\n",
    "plt.xlabel('height (in)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Heights');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you need to care about how many modes a distribution has? When analyzing bimodal distribution, using the mean or even the median can be misleading. It can easily be the case that there are very few observations close to the mean or median. This can be a problem is you are interpreting these measures of central tendency as \"typical\" values. This can happen when the overall population is made up of two or more heterogeneous groups.\n",
    "\n",
    "See https://www.nalp.org/startingsalarydistributionclassof2009 for an example of a bimodal distribution where normal descriptive statistics are misleading. This website shows the distribution of starting salaries for lawyers in 2009. For lawyers, salaries are typically very high (for those that get full-time positions) or very low (for those that can only secure part-time employment). As a result, the salaries follow a bimodal distribution and almost no one makes the mean or median salary.\n",
    "\n",
    "<img src=\"images/2009_bimodal_in_color_for_web.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Spread\n",
    "\n",
    "**Goal:** Give an idea of how similar or varied the observations in the dataset are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range\n",
    "\n",
    "The range measures how \"wide\" a dataset is. It depends only on the largest and smallest observations, so it is highly influenced by outliers.\n",
    "\n",
    "$$ \\text{range} = \\text{maximum observation} - \\text{minimum observation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know who has the largest salary, you can use the `nlargest` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.nlargest(1, 'salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar for the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers.nsmallest(1, 'salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the range, you can subtract the minimum value from the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].max() - lakers['salary'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the highest paid and lowest paid Laker is over \\$36 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.plots import range_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_plot(lakers['salary'], bins = 25)\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Salaries');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about for heights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_plot(lakers['height_inches'])\n",
    "plt.xlabel('height (in)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of Lakers\\' Heights');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is less than a foot of difference between the tallest and shortest players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and Standard Deviation\n",
    "\n",
    "The range of a dataset gives a quick glance at how varied a dataset is. It does have a major drawback, though, in that it only depends on two data points: the largest and smallest. What if you want to consider the entire dataset?\n",
    "\n",
    "You could start by looking at the deviations from the mean. That is, we could look at the difference between a player's salary and the mean salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, fixed\n",
    "from nssstats.plots import deviation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(deviation_plot, Player = lakers['player'], df = fixed(lakers));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this deviation is positive if the player makes an above-average salary and negative if he makes a below-average salary.\n",
    "\n",
    "Now, what if you want to look at how large these deviations are on average? There is one problem with simply taking the average of the deviations: if you were to sum the deviations, you would get zero, meaning that, on average, the deviation is zero.\n",
    "\n",
    "So, if you want a way to consider the average deviation, you must get rid of the negatives. One option is to simply take the absolute value, but the math works out nicer if instead you square the deviations.\n",
    "\n",
    "That is, for each datapoint $x_i$, look at the squared deviation $(x_i - \\mu)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now take the mean of these squared deviations, you get what is called the **variance**. Note that this formula is only valid if you are looking at a *population*. You'll see the difference for a sample shortly.\n",
    "$$\\text{Population Variance: } \\sigma^2 = \\frac{\\sum\\limits_{i = 1}^n(x_i - \\mu)^2}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one problem now: the variance is is squared units, not in our original unit. If we want to convert it to the starting units, you can take the square root and obtain what is called the **standard deviation**:\n",
    "\n",
    "$$\\text{Population Standard Deviation: } \\sigma = \\sqrt{\\sigma^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with sample, there is a small modification that must be done to calculate the variance and standard deviation. Instead of dividing the $n$, the number of observations, you instead divide by $n-1$:\n",
    "\n",
    "$$\\text{Sample Variance: } s^2 = \\frac{\\sum\\limits_{i = 1}^n(x_i - \\bar{x})^2}{n - 1}$$\n",
    "\n",
    "$$\\text{Sample Standard Deviation: } s = \\sqrt{s^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the difference? When working with a sample, you divide by $n - 1$, that is, one less than the number of observations. Informally, the reason that you do this is that you are trying to approximate the population variance. You want to estimate the deviation from the mean, but at the same time, you don't know the true population mean to start with, only an estimate from the sample ($\\bar{x}$). So you are making an estimate using an estimate. To compensate for this, you need to inflate your estimate of the variance slightly, by dividing by $n - 1$ instead of $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, most function that calculate the variance or standard deviation will assume that you are looking at a sample. However, in this case, you have the entire population, so you need to adjust it. If you are using `pandas` methods, you can specify `ddof = 0`, which sets the \"delta degrees of freedom\", or the amount that the \"degrees of freedom\" differ from the number of observations, to be 0.\n",
    "\n",
    "If you are calculating the standard deviation of a sample, you need to use `ddof = 1` (which is the default behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].var(ddof = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].std(ddof = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the details of the calculation to try and get a better understanding of what it is measuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['deviation_from_mean'] = lakers['salary'] - lakers['salary'].mean()\n",
    "lakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will square each salary's distance from the mean so that you are looking at distances as positive variations whether they are above or below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['squared_deviation'] = lakers['deviation_from_mean']**2\n",
    "lakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean value for the squared deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['squared_deviation'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next find the square root of the average squared deviation to get the deviation in absolute terms - regardless of whether is is above or below the mean - _and_ in the original units of measurment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(lakers['squared_deviation'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.plots import std_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_plot(lakers['salary'], bins = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the standard deviation for heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_plot(lakers['height_inches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Standard Deviation (Coefficient of Variation)\n",
    "\n",
    "What you may have noticed is that there is the standard deviation is salaries is much higher than that of heights. This has to do with the fact that the standard deviation is measured on the same scale as the variable at hand. So since salaries are measured in the hundreds of thousands or millions, whereas height is measured in the tens, it is not surprising that there is such a difference in standard deviations.\n",
    "\n",
    "However, there is a way to compare distributions in a unitless way. Namely, the *coefficient of variation*. It is defined as the ratio of the standard deviation to the mean.\n",
    "\n",
    "$\\text{CV} = \\frac{\\sigma}{\\mu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For salaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lakers['salary'].std(ddof = 0) / lakers['salary'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that the standard deviation is 1.3 times as large as the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, there is very little variation in heights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $z$-scores\n",
    "\n",
    "Often, you are not as interested in understanding \"how much\", but instead \"how different from average?\". That is, you may wish to measure how \"unusual\" a particular observation is. \n",
    "\n",
    "A $z$-score allows you to answer this question, in terms of the number of standard deviations from the mean. It is *unitless*, which means that it does not depend on what is being measured and the scale of the measurements, but instead you can compare across different types of measurements.\n",
    "\n",
    "$$ z\\text{-score} = \\frac{\\text{observation} - \\text{mean}}{\\text{standard deviation}}$$\n",
    "\n",
    "A $z$-score of 1.4 says that an observation is 1.4 standard deviations larger than the average value, whereas a $z$-score of -2.8 says that an observation is 2.8 standard deviations lower than the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dataset is reasonably close to being normally-distributed (meaning that the histogram looks roughly bell-shaped), then it will be highly unusual for an observation to have a $z$-score with magnitude 3 or larger.\n",
    "\n",
    "**Warning**: Be cautious when using $z$-scores for small datasets, as the maximum $z$-score is limited to $\\frac{n−1}{\\sqrt{n}}$ For example, for our dataset of 15 observations, the maximum $z$-score is $\\frac{14}{\\sqrt{15}} \\approx 3.6$. \n",
    "\n",
    "Further complicating the situation is that the presence of an outlier can inflate the mean and standard deviation, especially for a small dataset, which further distorts the interpretation of $z$-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary_z-score'] = (lakers['salary'] - lakers['salary'].mean()) / lakers['salary'].std(ddof = 0)\n",
    "lakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['height_z-score'] = (lakers['height_inches'] - lakers['height_inches'].mean()) / lakers['height_inches'].std(ddof = 0)\n",
    "lakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can see that the heights of players on the team show (relatively) less dispersion. While Lebron James' salary is nearly 3 standard deviations above the average salary, the tallest player is only about 1.6 standard deviations above average.\n",
    "\n",
    "Further, you can see that the distributions of heights is much more symmetric, and does not have a tail to either side. At the same time, for this distribution, it is not really bell-shaped either, since you do not see a large number of players near the average height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Position\n",
    "\n",
    "Measures of position have to do with ranking where an observation is in the dataset with respect to all other values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quartiles and Quantiles/Percentiles\n",
    "\n",
    "You have already encountered a special case of quantiles and percentiles, in the form of the median. Recall that the median of a dataset is the middle observation, if the observations are placed in ascending order. Another way to view this is that the median separates the lower half of the dataset from the upper half.\n",
    "\n",
    "Instead of dividing a dataset into halves, **quartiles** divide a dataset into quarters. The **first quartile** separates the smallest quarter of observations from the highest three-quarters, the **second quartile**, aka the median, separates the smallest half of observations from the largest half of observations, and the **third quartile** separates the smallest three-quarters from the largest quarter of observations.\n",
    "\n",
    "Quartiles (and more generally, **quantiles**) can be calculated using the `quantile` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].quantile(q = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].quantile(q = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].quantile(q = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the quantiles to find the **interquartile range**, which is defined as the distance from the first to the third quartile. In a way, it is a trimmed version of the range, which is not as sensitive to extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].quantile(q = 0.75) - lakers['salary'].quantile(q = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.plots import iqr_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_plot(lakers['salary'], bins = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the middle half of observations all fall into a range of width $3.8 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_plot(lakers['height_inches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, you can look at the quantiles or percentiles. The $n$th percentile separtes the lowest $n$% of observations from the rest. For example, the 90th percentile divides the lowest 90% of observations from the highest 10%. \n",
    "\n",
    "To find percentiles, you can use the quantile function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].quantile(q = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers['salary'].quantile(q = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentiles can be used to identify unusual observations, or to trim outliers from a data set.\n",
    "\n",
    "If you want to understand how a variable is distributed, you have already seen how to use a histogram. An alternative type of plot that you can use is a **boxplot** (aka **box-and-whiskers plot**). This type of plot displays a box which starts at the first quartile and extend to the third quartile, with the second quartile marked. It also has whiskers that extend to last observations contained within the **outlier boundaries**. \n",
    "\n",
    "These boundaries are (usually) defined as being at 1.5 times the interquartile range below the first quartile and above the third quartile. Any points outside of the outiler boundaries are plotted individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lakers['salary']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the boxplot, we can detect that most salaries are less than \\$10 million, but that there are three players with very high salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lakers['height_inches']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are also good for comparing the distribution of a variable across two or more categories. For example, let's compare salaries for the Lakers to salaries for the Memphis Grizzlies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers_grizzlies = pd.read_csv('../data/Lakers_Grizzlies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakers_grizzlies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add an argument to tell seaborn how to divide the data into categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = lakers_grizzlies, x = \"team\", y = \"salary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this boxplot, you can see that the Grizzlies have a wider interquartile range and higher median salary with no points considered as outliers. This may be due in part to the NBA's salary cap, which limits the total payroll of each team. Since the Lakers have several extremely highly-compensated players, it leaves less money to go around to the remaining players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now look at a dataset consisting of a sample 1000 home appraisal values from Nashville/Davidson County in the year 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009 = pd.read_csv('../data/appraisal_sample.csv')\n",
    "\n",
    "houses_2009.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the measures of central tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then some measures of spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].max() - houses_2009['total_appr'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_plot(houses_2009['total_appr'], bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the standard deviation, remember to set the ddof = 1 since we are looking at a sample and not the entire population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_plot(houses_2009['total_appr'], bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_plot(houses_2009['total_appr'], bins = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen that there is a large range and standard deviation in this dataset. Find the potential outilers for this dataset by looking at $z$-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = houses_2009['total_appr'].mean()\n",
    "sigma = houses_2009['total_appr'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "houses_2009['z_score'] = (houses_2009['total_appr'] - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect those houses which have very large z-scores. To filter a dataframe, you can pass in an inequality inside a bracket, like in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "houses_2009[houses_2009['z_score'] > 3].sort_values('total_appr', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation\n",
    "\n",
    "As you have more observations, you can use more bins when creating our histogram. At a certain point, it becomes too \"spiky\", and you lose out on the true picture. An alternative strategy is to smooth out the histogram using what's called kernel density estimation.\n",
    "\n",
    "When creating a histogram, you are really performing **density estimation**. That is, you are trying to understand the **probability density function** which underlies the observed data.\n",
    "\n",
    "When doing statistical analysis, you are usually assuming that your dataset is only a sample from some larger population. A probability density function describes how likely you are to see observations within a certain range. Since you are only looking at a sample, you can't know what the true probability density function looks like, but you can use a variety of techniques to estimate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009['total_appr'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One downside to a histogram is that it can be highly sensitive to the choice of number of bins.\n",
    "\n",
    "Since this dataset has a handful of very large outliers, you will only look at home appraised at less than \\$1,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'].hist(bins = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you increase the number of bins, you get more and more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'].hist(bins = 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'].hist(bins = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But at a certain point, you are getting almost \"too much\" detail, and it is hard to know if you are actually seeing important aspects of the dataset or you are just seeing relics from the random sampling plus the binning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'].hist(bins = 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen, the histogram is extremely sensitive to the choice of bins. As an alternative, a Kernel Density Estimation is a way to \"smooth\" a histogram. One way you can get a density estimate is by using the `distplot` function from `seaborn`. This will create a histogram with a kde superimposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "sns.distplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want just the kernel density estimation, you can use the `kdeplot` instead. Kernel density estimation works by looking at the density of observations within a certain \"bandwidth\" around each point. you can adjust the bandwidth to see the effect on the density estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'], bw = 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'], bw = 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'], bw = 25000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'], bw = 100000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr'], bw = 500000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `kdeplot` will use a method to make a guess at a good bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(houses_2009[houses_2009['total_appr'] < 1000000]['total_appr']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compare the distribution of appraisal values across multiple years using both boxplots and violin plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_appraisal = pd.read_csv('../data/appraisal_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = nash_appraisal[nash_appraisal['total_appr'] < 1000000], \n",
    "               x = 'year', y = 'total_appr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Variables\n",
    "\n",
    "Recall that qualitative variables are those that fall into two or more levels. For qualitative variables, it is no longer meaningful to compute descriptive statistics such as mean or standard deviation.\n",
    "\n",
    "The most interesting information in regard to qualitative variables is the number of observations per level.\n",
    "\n",
    "This can be displayed in a **frequency table**, which shows a count of observations per category.\n",
    "\n",
    "For this example, we'll look at data from the 2018 Central Park Squirrel Census, which can be obtained from https://data.cityofnewyork.us/Environment/2018-Central-Park-Squirrel-Census-Squirrel-Data/vfnx-vebw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squirrels = pd.read_csv('../data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv')\n",
    "squirrels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that one of the variables in the dataset is the primary fur color. If you want to see how many squirrels there were for each fur color, you can use the `value_counts` method from `pandas` to create a frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squirrels['Primary Fur Color'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that gray squirrels are by far the most common squirrel spotted in Central Park in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not interested in the *number* of observations of each level, but instead the *proportion* of observations in each category, you can add the `normalize = True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squirrels['Primary Fur Color'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to visualize the frequency per color, you can create a **bar plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squirrels['Primary Fur Color'].value_counts().plot(kind = 'bar')\n",
    "plt.xticks(rotation = 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's your turn. Look at the frequency and relative frequency of the \"Foraging\" column. Create a bar plot to display what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
