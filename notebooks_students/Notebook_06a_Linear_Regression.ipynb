{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "In this notebook, you'll see how to create a linear regression model using the `scikit-learn` library.\n",
    "\n",
    "You will be using a cleaned up version of the auto mpg dataset, with the goal being to predict a car's mpg based on the other attributes of that car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('../data/auto_mpg_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict mpg based off of cylinders, displacement, horsepower, acceleration, and origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, you can do some exploration to see how the variables are related to mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at the numeric predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(x = ['displacement', 'horsepower', 'weight', 'acceleration', 'cylinders'])\n",
    "def make_scatter(x):\n",
    "    cars.plot(kind = 'scatter', x = x, y = 'mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some categorical predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(x = ['origin', 'model_year'])\n",
    "def make_box(x):\n",
    "   sns.boxplot(data = cars, x = x, y = 'mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Let's start with a simple linear regression. This means that we'll only use a single predictor variable. Let's use the `weight` feature to try and predict `mpg`.\n",
    "This means that our regression equation will look like\n",
    "$$ \\text{mpg} = \\beta_0 + \\beta_1\\cdot(\\text{weight}) + \\epsilon$$\n",
    "\n",
    "Here, $\\beta_0$ and $\\beta_1$ are the coefficients which need to be solved for and $\\epsilon$ represents the error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars[['weight']]\n",
    "y = cars['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building models, you are often interested in the **predictive power** of the model. You are not interested in how well the model predicts on data that it has already seen, but instead on how well it generalizes to new, unseen data.\n",
    "\n",
    "To evaluate this, you will set aside a portion of the full dataset as your _test set_. The remaining portion, called the _training set_ will be used to fit the model. \n",
    "\n",
    "Random state is an arbitrary value you assign so that you can reproduce your work or so that others running your code will get the same result as you. It is used when there is a random process, such as randomly splitting the data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to create a LinearRegression instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then fit it on the training data. _Fitting a model_ means that you are showing your model labeled instances so that it can learn about the relationship between the predictor variables and the target variable. In the case of linear regression, the model needs to learn which coefficients best describe the relationship between predictors and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the coefficients $\\beta_0$ and $\\beta_1$ using the `intercept_` and `coef_` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our regression equation is given by\n",
    "\n",
    "$$ \\text{mpg} = 46.1665399 - 0.00762584\\cdot(\\text{weight}) + \\epsilon$$\n",
    "\n",
    "This means that a one unit increase in weight is associated with a 0.00762584 drop in mpg. Put another way, adding 100 to weight is associated with a drop in mpg of 0.762584."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the regression line with the data to see how well it appears to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[np.min(X_train.weight)], [np.max(X_train.weight)]])\n",
    "y = lr.predict(x)\n",
    "\n",
    "plt.scatter(x = X_train.weight, y = y_train)\n",
    "plt.plot(x, y, color = 'red', linewidth = 3, label = 'regression line')\n",
    "plt.legend()\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate some metrics for the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the $R^2$ score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train, lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you're interested in predictions, you shouldn't care as much about how the model does on the training data. Instead, you should be interested in how it does on data it has not yet seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = X_test.weight, y = y_test)\n",
    "plt.plot(x, y, color = 'red', linewidth = 3, label = 'regression line')\n",
    "plt.legend()\n",
    "plt.title('Test Data')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Comment on what you see from this plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above plot, it appears that maybe a straight line is not the best model to use. Instead, you could try to fit a parabola, or a degree 2 polynomial based on weight. This will turn our regression equation into \n",
    "$$ \\text{mpg} = \\beta_0 + \\beta_1\\cdot(\\text{weight}) + \\beta_2\\cdot(\\text{weight})^2 + \\epsilon$$\n",
    "\n",
    "One way to do this is to use `PolynomialFeatures` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 2)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "X_test_poly = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures creates a column for the intercept, so we can specify `fit_intercept = False` when we create our new `LinearRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = LinearRegression(fit_intercept = False)\n",
    "poly_reg.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(start = np.min(X_train.weight), stop = np.max(X_train.weight)).reshape(-1, 1)\n",
    "x_poly = poly.transform(x)\n",
    "y = poly_reg.predict(x_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = X_train.weight, y = y_train)\n",
    "plt.plot(x, y, color = 'red', linewidth = 3, label = 'regression line')\n",
    "plt.legend()\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = X_test.weight, y = y_test)\n",
    "plt.plot(x, y, color = 'red', linewidth = 3, label = 'regression line')\n",
    "plt.legend()\n",
    "plt.title('Test Data')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on how the degree 2 curve compares to the original.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, poly_reg.predict(X_test_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, poly_reg.predict(X_test_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "There is no reason that you are restricted to using only weight to predict mpg. You can also include other features in you model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we want to incorporate some of the other features. We'll use `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, and `origin`.\n",
    "\n",
    "First, let's drop the features you won't be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.drop(columns = ['car_name', 'model_year'])\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to deal with the `origin` column, which is a categorical variable. \n",
    "\n",
    "To use a categorical variable in your model, you'll need to transform it into a \"dummy\" numerical variable. You will do this by creating new indicator columns for all of the different possible levels of these variables. You can accomplish this by using the `get_dummies` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.get_dummies(cars, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_dummies()` method created two columns out of the original origin column. In these columns, the corresponding value is marked with a 1, and all other values are maked with a 0.\n",
    "\n",
    "**Question: Why do we not need a column `origin_American`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to build a linear model to predict `mpg` based on our predictors. This will look like\n",
    "\n",
    "$$\\text{mpg} = $$ $$\\beta_0 + \\beta_1\\cdot(\\text{cylinders}) + \\beta_2\\cdot(\\text{horsepower}) + \\beta_3\\cdot(\\text{weight}) + \\beta_4\\cdot(\\text{acceleration}) + \\beta_5\\cdot(\\text{origin_European}) + \\beta_6\\cdot(\\text{origin_Japanese}) + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors:\n",
    "X = cars.drop(columns = 'mpg')\n",
    "\n",
    "# Response:\n",
    "y = cars['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_multiple = LinearRegression()\n",
    "lr_multiple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate how well the model performs. You can no longer plot mpg vs. predictor with the regression line, since we now have 6 predictor variables. However, if you want a visual showing the performance of the model, you can plot the predicted value against the true value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_multiple.predict(X_test)\n",
    "\n",
    "x_min = min(y_pred.min(), y_test.min())\n",
    "x_max = max(y_pred.max(), y_test.max())\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([x_min, x_max], [x_min, x_max])\n",
    "\n",
    "plt.xlabel('true value')\n",
    "plt.ylabel('predicted value')\n",
    "\n",
    "plt.title('Performance on the Test Set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: What does this plot tell you?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How does the performance of this model compare to the models above? Do you have any ideas why this may be the case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to understand _how_ the model is making predictions? Since you are using a linear model, looking at coefficients can help you understand the model. The intercept and coefficients can be accessed from our trained model, `lr`.  \n",
    "\n",
    "The code in the following cell extracts the coefficients and converts the result into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({'variable': ['intercept'] + list(X.columns),\n",
    "                             'coefficient': [lr_multiple.intercept_] + list(lr_multiple.coef_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the continuous variables, the coefficient represents the change in mpg that would occur for a one-unit change in the corresponding predictor, _if all other predictors are held constant_.\n",
    "\n",
    "For example, our coefficients show that for every one unit increase in horsepower, all other variables held constant, there is a drop in mpg of 0.093286."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one strange value that stands out. It seems that according to the model, increasing displacement will increase mpg. However, if you look at the scatterplot earlier in this notebook, it seems that there is a negative association between the two variables. Cars with higher than average displacement (or engine size) tend to have lower than average mpg.\n",
    "\n",
    "This can happen for a number of reasons, but the cause for an unexpected coefficient sign is due to correlations with other predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.plot(kind = 'scatter', x = 'horsepower', y = 'displacement');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, for example, that displacement is strongly correlated with horsepower. One possible explanation for the positive coefficient on horsepower is that the effect of displacement has already been captured by the horsepower.\n",
    "\n",
    "All of this to say that when you have correlated predictors, you need to exercise caution when interpreting them. It is always a good idea to do thorough exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the categorical predictors? Look specifically at the origin variable. Since you dropped the origin_American variable when dummyizing, you can interpret the other two as the change in mpg from changing a car's origin to either European or Japanese, keeping all other variables fixed.\n",
    "\n",
    "The model is telling you that, all other variables held fixed, a European car will tend to get about 2.38 mpg higher than an American car, and a Japanese car will tend to get about 3.21 mpg higher than an American car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Polynomial Features and Interaction Terms\n",
    "\n",
    "We have seen that it looks like some of the relationships between our predictors and the target may not necessarily be linear. Also, we did not account for the fact taht there may be **interactions** between predictors.\n",
    "\n",
    "To add some more flexibility to our model, we can add some polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create features of degree 2 and below. That means that we'll have all of our original predictors, plus all terms like `cylinders^2`, `displacement^2`, etc., as well as all products like `cylinders * displacement` and `horsepower * displacement`.\n",
    "\n",
    "The terms which are the products of two features are called **interaction terms** and are useful if, for example, displacement affects mpg differently for American cars compared to European cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though we included only degree 2 and below, we have added 29 new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = LinearRegression()\n",
    "\n",
    "pr.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pr.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_min = min(y_pred.min(), y_test.min())\n",
    "x_max = max(y_pred.max(), y_test.max())\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([x_min, x_max], [x_min, x_max])\n",
    "\n",
    "plt.xlabel('true value')\n",
    "plt.ylabel('predicted value')\n",
    "\n",
    "plt.title('Performance on the Test Set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the coefficients for each feature, you need to clean up the variable names a bit becuse we lose our original column names when using PolynomialFeatures. Here's a helper function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_names(feature_name):\n",
    "    if feature_name == '1':\n",
    "        return 'intercept'\n",
    "    for i, col_name in enumerate(X_train.columns):\n",
    "        feature_name = feature_name.replace('x' + str(i), col_name)\n",
    "        \n",
    "    return feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = poly.get_feature_names()\n",
    "col_names = [fix_names(x) for x in col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({'variable': col_names,\n",
    "                             'coefficient': list(pr.coef_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
