{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression\n",
    "\n",
    "You have seen how to build a linear regression model. A linear regression model is useful if predicting a *numeric* outcome. What if you want to predict a *categorical* outcome?\n",
    "\n",
    "In this notebook, you'll learn how you can use a **logistic regression** model for just such a situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will be looking at data related to Duchenne Muscular Dystrophy (DMD). DMD is a genetically transmitted disease, passed from a mother to her children. Affected female offspring usually suffer no apparent symptoms, male offspring with the disease die at young age. Although female carriers have no physical symptoms they tend to exhibit elevated levels of certain serum enzymes or proteins. \n",
    "\n",
    "The dystrophy dataset contains 209 observations of 75 female DMD carriers and 134 female DMD non-carriers. It includes 6 variables describing the age of the female and the serum parameters serum marker creatine kinase (CK), serum marker hemopexin (H), serum marker pyruvate kinase (PK) and serum marker lactate dehydroginase (LD). The serum markers CK and H may be measured rather inexpensively from frozen serum, while PK and LD require fresh serum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dystrophy = pd.read_csv('../data/dystrophy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dystrophy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by doing some prelimilary exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = dystrophy, x = 'Class', y = 'LD')\n",
    "plt.ylabel('lactate dehydroginase (LD)', fontsize = 12)\n",
    "plt.xlabel('')\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.title('Distribution of LD Values', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, in the dataset, that carriers tend to have higher LD values. However, there is some overlap between carriers and non-carriers in the middle values. There is not a single cutoff you can use to classify a person as a carrier or non-carrier.\n",
    "\n",
    "Now you're going to look at the above plot in a different way. To accomplish this, you're going to encode the normal/carrier classification numerically. Mark carriers with a 1 and non-carriers with a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dystrophy['carrier'] = (dystrophy.Class == 'carrier').astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view our data as a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dystrophy.plot(x = 'LD', y = 'carrier', kind = 'scatter', edgecolor = 'black', s = 50)\n",
    "plt.title('Carrier vs. LD')\n",
    "plt.ylabel('carrier', fontsize = 12)\n",
    "plt.xlabel('LD', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is overlap between carriers and non-carriers, you would probably be best off not making a simple prediction of carrier/non-carrier, but instead predicting the likelihood or probability that a female is a carrier.\n",
    "\n",
    "From what is seen in the plots, females with higher LD values look more likely to be carriers that those with lower values.\n",
    "\n",
    "Between 175 and 225, it is not clear if a person is a carrier or not, so you might be best off assigning a probability close to 0.5 for people in that range.\n",
    "\n",
    "But how does one fit such a model? You can start by (naively) trying to fit a linear regression model. Recall that you need to split the data into train and test sets prior to building a model so that you can assess how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dystrophy[['LD']]\n",
    "y = dystrophy.carrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing the train/test split, you will stratify on the target (y), so that you get an equal mix of carriers and non-carriers in the training and test sets. Again, use a random state so that you can reproduce your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify = y, \n",
    "                                                    random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients tell you \n",
    "$$P(carrier) = 0.00438894\\cdot(LR) - 0.502602$$\n",
    "\n",
    "That is, increasing LR increases the chances that a person is a carrier. Next, plot the resulting curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lin_reg.coef_[0]\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "x = np.linspace(start = X.min(), stop = X.max())\n",
    "y = m*x + b\n",
    "\n",
    "alpha = 0.75\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "dystrophy.plot(x = 'LD', y = 'carrier', kind = 'scatter', ax = ax, edgecolor = 'black', s = 50, alpha = alpha)\n",
    "plt.plot(x, y, color = 'red')\n",
    "\n",
    "plt.title('Carrier vs. LD')\n",
    "plt.ylabel('carrier', fontsize = 12)\n",
    "plt.xlabel('LD', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has a big problem: probabilities must be between 0 and 1, but this equation has no guarantee of outputting values between 0 and 1.\n",
    "\n",
    "In fact, you can see that for some values, there are predictions less than 0 or greater than 1.\n",
    "\n",
    "Another problem is that it assumes a fixed change in LD will have a fixed effect on the probability. That is, a change from 60 to 70 will have the same impact as a change from 250 to 260.\n",
    "\n",
    "How can you improve the model?\n",
    "\n",
    "One approach is to \"squash\" the output between 0 and 1. A common way to do this is using the **logistic function**:\n",
    "\n",
    "$$l(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num = 250)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('The Logistic Function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the logistic function maps large positive inputs to values close to 1, large negative inputs to values close to 0, and inputs close to zero get mapped to values close to 0.5.\n",
    "\n",
    "To use the logistic function, you will take the result of a linear model and feed it into the logistic function, so instead of the model looking like:\n",
    "\n",
    "$$P(carrier) = \\beta_1\\cdot(LD) + \\beta_0$$\n",
    "\n",
    "it will look like\n",
    "\n",
    "$$P(carrier) = \\frac{1}{1 + e^{-(\\beta_1\\cdot(LD) + \\beta_0)}}$$\n",
    "\n",
    "**Important:** The coefficients for this new model will be different from the model above. If you want to fit a logistic regression model, you can use the LogisticRegression module from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "dystrophy['label'] = le.fit_transform(dystrophy.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, once fit, we can inpect the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the model is\n",
    "\n",
    "$$P(carrier) = \\frac{1}{1 + e^{-(0.03065\\cdot(LD) - 6.6795)}}$$\n",
    "\n",
    "Increasing LD results in a higher probability of a person being a carrier.\n",
    "\n",
    "See what the model looks like now. First, you'll look at the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lr.coef_[0]\n",
    "b = lr.intercept_\n",
    "\n",
    "alpha = 0.75\n",
    "\n",
    "x_log = np.linspace(start = X.LD.min(), stop = X.LD.max())\n",
    "y_log = 1 / (1 + np.exp(-(m*x_log + b)))\n",
    "\n",
    "df = dystrophy.loc[X_train.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "df.plot(x = 'LD', y = 'carrier', kind = 'scatter', ax = ax, alpha = alpha, s = 50, \n",
    "        edgecolor = 'black')\n",
    "plt.plot(x_log, y_log, color = 'red', label = 'Probability of Carrier')\n",
    "\n",
    "plt.title('Carrier vs. LD, Test Set')\n",
    "plt.ylabel('carrier', fontsize = 12)\n",
    "plt.xlabel('LD', fontsize = 12)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lr.coef_[0]\n",
    "b = lr.intercept_\n",
    "\n",
    "alpha = 0.75\n",
    "\n",
    "x_log = np.linspace(start = X.LD.min(), stop = X.LD.max())\n",
    "y_log = 1 / (1 + np.exp(-(m*x_log + b)))\n",
    "\n",
    "df = dystrophy.loc[X_test.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "df.plot(x = 'LD', y = 'carrier', kind = 'scatter', ax = ax, alpha = alpha, s = 50, edgecolor = 'black')\n",
    "plt.plot(x_log, y_log, color = 'red', label = 'Probability of Carrier')\n",
    "\n",
    "plt.title('Carrier vs. LD, Test Set')\n",
    "plt.ylabel('carrier', fontsize = 12)\n",
    "plt.xlabel('LD', fontsize = 12)\n",
    "plt.legend(loc = 'right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how good is this model?\n",
    "\n",
    "The red sigmoid or s-curve shows the predicted probability associated with a given level of LD.\n",
    "\n",
    "There are a number of ways to assess such a model. One option is to determine how \"calibrated\" the model is. That is, if you look at people who you say have a 25% probability of being a carrier, you want about 25% of them to be carriers.\n",
    "\n",
    "Using this idea, you can build a **calibration curve**. This curve is constructed by:\n",
    "1. Binning the data into groups based on predicted probabilities.\n",
    "2. Computing the average predicted probability for each group.\n",
    "3. Computing the observed proportion for each group, along with a confidence interval (usually a 95% confidence interval).\n",
    "4. Plot the observed probabilities and confidence intervals against the average probabilities for each group.\n",
    "\n",
    "Ideally, the plotted points should fall close to the line y = x (because predicted probabilities should be similar to observed probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.plots import calibration_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, you need to pass in the true labels as well as the predicted probabilities. We can access the predicted probabilities using the `predict_proba` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = lr.predict_proba(X_test)\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually outputs two columns. The first column is the predicted probability of _not_ being a carrier and the second give the predicted probability of being a carrier. Notice that the two columns sum to 1, as they should since they represent probabilities.\n",
    "\n",
    "The calibration curve function expects **just** the probabilities of being a carrier, so we need to slice `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = y_proba[:,1]\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_curve(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is decently well calibrated. All of our confidence intervals intersect the line y = x , but the second and third point estimates are a bit low. Also, we have a smallish sample size, so we need to be cautious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we could look at performance is to calculate how well our model **discriminates** between positive classes and negative classes.\n",
    "\n",
    "That is, does the model tend to assign higher probabilities to positive observations compared to negative observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.plots import predicted_probability_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probability_plot(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the highest probabilities are assigned to those cases which are carriers. However, there are quite a few carriers who are assigned a low probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closely related to the plot of the predicted probabilities is the Receiver Operator Characteristic Curve (ROC).\n",
    "\n",
    "This curve shows the tradeoff between correctly classifying those who are carriers vs. incorrectly classifying those who are not carriers as you adjust the threshold for how sure you have to be to predict that a person is a carrier. The axes on the ROC are the **true positive rate**, which measures the proportion of carriers who are correctly predicted as such and the **false positive rate**, which measures the proportion of non-carriers who are incorrectly predicted as being a carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.roc import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "roc_curve(y_test, y_proba);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider, fixed\n",
    "from nssstats.roc import tpr_fpr, roc_interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widget below shows how the ROC curve is drawn. The threshold represents how high of a predicted probability you need before predicting that someone is a carrier. That is, any points to the right of the threshold will be predicted to be carriers. \n",
    "\n",
    "At high threshold values, the model mostly has carriers to the right. By setting a high threshold, you miss out of some of the carriers.\n",
    "\n",
    "However, if you lower the threshold, you will start to misclassify more and more non-carriers. This demonstrates the tradeoff between the true positive rate and the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = tpr_fpr(y_test, y_proba)\n",
    "\n",
    "interact(roc_interact, threshold = FloatSlider(value = 0.5, min = 0, max = 1, step = 0.01, continuous_update = False),\n",
    "         y_true = fixed(y_test),\n",
    "         y_prob = fixed(y_proba),\n",
    "         tpr = fixed(tpr),\n",
    "         fpr = fixed(fpr),\n",
    "         alpha = fixed(0.6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model which discriminates between positive and negative classes really well will be able to increase the True Positive Rate while avoiding increasing the False Positive Rate at the same time.\n",
    "\n",
    "You can quantify the ability of a model to discriminate by calculating the area under the ROC curve. Areas closer to 1 indicate a model which is better at discriminating between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "roc_curve(y_test, y_proba, area = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate this area using the roc_auc_score function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, take a look at how good the predictions from this model are. You can get predictions using the `lr.predict` method. This returns a predicted class: 0 for non-carrier and 1 for carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe the plot below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lr.coef_[0]\n",
    "b = lr.intercept_\n",
    "\n",
    "threshold = -b/m\n",
    "\n",
    "x_log = np.linspace(start = X.LD.min(), stop = X.LD.max())\n",
    "y_log = 1 / (1 + np.exp(-(m*x_log + b)))\n",
    "\n",
    "alpha = 0.75\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "\n",
    "df = dystrophy.loc[X_test.index]\n",
    "\n",
    "df[y_pred == df.carrier].plot(x = 'LD', y = 'carrier', kind = 'scatter', ax = ax, color = 'blue', \n",
    "               edgecolor = 'black', s= 50, label = 'correct prediction', alpha = alpha) \n",
    "df[y_pred != df.carrier].plot(x = 'LD', y = 'carrier', kind = 'scatter', ax = ax, color = 'red', \n",
    "               edgecolor = 'black', s= 50, label = 'incorrect prediction', alpha = alpha) \n",
    "plt.plot(x_log, y_log, color = 'red', label = 'predicted probability')\n",
    "\n",
    "plt.vlines(x = threshold, ymin = 0, ymax = 1, linestyle = '--', label = 'threshold')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "\n",
    "plt.title('Carrier vs. LD')\n",
    "plt.ylabel('carrier', fontsize = 12)\n",
    "plt.xlabel('LD', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient way to analyze these predictions is through the use of a **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nssstats.cm import cm_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_analysis(y_true, y_pred, labels, filename = None, ymap=None, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      figsize:   the size of the figure plotted.\n",
    "\n",
    "    Modified from https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7#file-plot_confusion_matrix-py\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(object)\n",
    "    annot_kws = {'fontsize': 12, 'fontweight' : 'bold'}\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j == 0:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = 'True Negatives:\\n %.1f%%\\n%d/%d' % (p, c, s) \n",
    "            elif i == j == 1:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = 'True Positives:\\n %.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif i == 0 and j == 1:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = 'False Positives:\\n %.1f%%\\n%d/%d' % (p, c, s)\n",
    "            else:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = 'False Negatives:\\n %.1f%%\\n%d/%d' % (p, c, s)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax, linewidths = 2, linecolor = 'black', annot_kws = annot_kws,\n",
    "               cmap = 'Blues')\n",
    "    plt.ylabel('Actual', fontsize = 16, fontweight = 'bold')\n",
    "    plt.xlabel('Predicted', fontsize = 16, fontweight = 'bold')\n",
    "    ax.set_xticklabels(labels, fontsize = 14, ha = 'center')\n",
    "    ax.set_yticklabels(labels, fontsize = 14, va = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = dystrophy.loc[X_test.index]\n",
    "cm_analysis(le.inverse_transform(df.label),\n",
    "            le.inverse_transform(1 - y_pred), labels = ['normal', 'carrier'],\n",
    "           figsize = (7,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might you try and improve this model? There are several other measurment variables, so you could try including some of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dystrophy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dystrophy.loc[X_train.index, ['CK', 'H', 'PK', 'LD']]\n",
    "X_test = dystrophy.loc[X_test.index, ['CK', 'H', 'PK', 'LD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = lr.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_curve(y_test, y_proba, n_bins = 5, strategy = 'quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it is well calibrated at the top and bottom of the predicted probabilities, but is having some difficulty in the middle range. Now, let's look at how well it discriminates between carriers and non-carriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probability_plot(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = tpr_fpr(y_test, y_proba)\n",
    "\n",
    "interact(roc_interact, threshold = FloatSlider(value = 0.5, min = 0, max = 1, step = 0.01, continuous_update = False),\n",
    "         y_true = fixed(y_test),\n",
    "         y_prob = fixed(y_proba),\n",
    "         tpr = fixed(tpr),\n",
    "         fpr = fixed(fpr),\n",
    "         alpha = fixed(0.6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, y_proba, area = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a pretty good AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "df = dystrophy.loc[X_test.index]\n",
    "\n",
    "cm_analysis(le.inverse_transform(df.label),\n",
    "            le.inverse_transform(1 - y_pred), labels = ['normal', 'carrier'],\n",
    "           figsize = (4,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, take a look at the coefficients to understand how each feature influences the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({'feature': X_train.columns, 'coefficient': lr.coef_[0]})\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the coefficients being positive indicates that higher levels of any one of them will tend to indicate a greater chance of being a carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
